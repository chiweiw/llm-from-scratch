# 知乎个人主页回答采集器 - 需求文档

## 项目信息

- **项目名称**: 知乎个人主页回答采集器 (Zhihu-User-Spider)
- **版本号**: v1.1
- **日期**: 2026-01-26
- **状态**: 已开发

---

## 1. 项目背景与目标

### 1.1 背景

需要获取特定知乎用户（大V或特定答主）过往发布的所有"回答"数据，用于本地归档、内容分析或建立个人知识库。

### 1.2 核心目标

开发一个基于 Python 的本地自动化脚本，能够：
- 输入指定用户的 ID
- 自动化遍历其所有回答页面
- 绕过基础反爬机制（x-zse-96 签名、验证码）
- 将数据完整保存为 CSV/Excel 格式
- **支持断点续传，异常后可从上次位置继续**
- **支持数量限制，可配置爬取指定数量的回答**

---

## 2. 技术选型 (Technical Stack)

基于"抗检测性强、开发成本低"的原则，确定以下技术栈：

- **编程语言**: Python 3.8+
- **核心框架**: DrissionPage (ChromiumPage 模式)
  - 理由: 能够接管浏览器，无 WebDriver 特征，自带数据包监听功能，适合处理知乎的动态加载
- **数据处理**: Pandas
- **运行环境**: 本地 Windows/macOS (非服务器环境)
- **网络策略**: 本地宽带 / 手机热点 (利用飞行模式切换 IP)

---

## 3. 功能需求 (Functional Requirements)

### 3.1 输入模块

程序应支持在代码配置区或命令行输入目标用户的 url_token (即主页 URL /people/ 后的字符串)。

- 示例 ID: kaifulee, zhang-san
- **新增**: 可配置爬取数量限制（MAX_ANSWER_COUNT），0 表示无限制

### 3.2 登录与鉴权模块

**交互方式**: 采用 Semi-Automatic (半自动) 模式。

**逻辑**:
1. 启动浏览器，检测是否已登录
2. 若未登录，弹出浏览器窗口，保留 30-60 秒等待用户手动扫码登录
3. 登录成功后，自动保存 Cookie/User Data 到本地，实现下次运行免登录

### 3.3 数据采集模块 (核心)

**采集模式**: 监听网络数据流 (Network Listening/MITM)，而非 DOM 解析。

**监听目标**: 拦截包含 api/v4/members/{user_id}/answers 的响应包。

**翻页逻辑**:
1. 模拟真人点击"下一页"按钮
2. 必须包含随机等待: 每次翻页间隔随机 3-6 秒
3. **断点续传**: 记录已爬取的回答ID和当前页码到 progress.json，程序中断后再次运行时自动从上次位置继续

### 3.4 字段定义

对于每一条回答，需采集以下字段：

| 字段名 | 字段描述 | 数据来源 (JSON Key) |
|--------|---------|-------------------|
| Question Title | 问题标题 | question.title |
| Answer ID | 回答唯一ID | id |
| Excerpt | 回答摘要/预览 | excerpt |
| Content | 完整HTML内容(可选) | content |
| Vote Count | 赞同数 | voteup_count |
| Comment Count | 评论数 | comment_count |
| Create Time | 发布时间 | created_time (需转为日期格式) |
| URL | 回答链接 | 拼接 question.id + id |

### 3.5 输出模块

- **格式**: .csv (UTF-8-SIG 编码，防止 Excel 乱码)
- **命名**: {user_id}_answers_{timestamp}.csv
- **频率**: 每抓取一页（20条）即写入一次文件（或追加写入），防止崩溃导致数据全丢

### 3.6 数量限制功能 (新增)

- **配置项**: MAX_ANSWER_COUNT (在 config.py 中配置)
- **默认值**: 0 (表示无限制，爬取所有回答)
- **功能**: 当配置为正整数时，爬取到指定数量后自动停止
- **应用场景**: 测试、调试或只需要部分数据时使用

### 3.7 断点续传功能 (新增)

- **进度文件**: progress.json (保存在项目根目录)
- **记录内容**:
  - user_id: 用户ID
  - answer_ids: 已爬取的回答ID列表
  - current_page: 当前页码
  - last_update: 最后更新时间
- **工作流程**:
  1. 程序启动时自动加载进度文件
  2. 跳过已爬取的回答ID
  3. 从上次中断的页码继续翻页
  4. 每页数据保存后自动更新进度文件

---

## 4. 非功能需求 (Non-Functional Requirements)

### 4.1 反爬虫对抗策略 (Anti-Scraping)

**浏览器指纹**: 使用 DrissionPage 默认配置，无需额外隐藏 WebDriver。

**行为模拟**:
- 禁止匀速翻页
- 禁止无间隔连续请求

**风控处理**:
- **验证码**: 遇到滑块或点选验证码时，程序不应报错退出，而应暂停挂起，并发出提示音，等待人工手动在浏览器上完成验证，随后程序自动恢复运行
- **403 Forbidden**: 若连续出现 403 错误，程序暂停并提示用户"请切换 IP (开关飞行模式)"，待用户确认后继续

### 4.2 性能指标

- **速度**: 不追求极速，以稳定为主。预期速度约 300-500 条/分钟（受限于随机延迟）
- **资源**: 单线程运行，内存占用 < 500MB

---

## 5. 异常处理流程 (Exception Handling)

| 异常场景 | 处理逻辑 |
|---------|---------|
| 网络超时 | 重试 3 次，每次间隔 5 秒。若仍失败，记录当前页数日志并退出 |
| 找不到"下一页"按钮 | 判断是否已到达最后一页。若是，正常结束；若否，尝试滚动页面重试 |
| JSON 解析失败 | 跳过该条数据，记录错误日志，不中断主程序 |
| 登录失效 | 提示"Cookie 已失效"，清除本地 User Data 文件夹，强制重新扫码 |
| 程序中断 | 自动保存已采集数据和进度文件，确保数据不丢失 |

---

## 6. 开发与交付计划

### 6.1 交付物

- **spider.py**: 核心爬虫程序
- **config.py**: 配置文件
- **requirements.txt**: 依赖包列表
- **README.md**: 使用说明文档
- **test_spider.py**: 单元测试
- **progress.json**: 断点续传进度文件（运行时自动生成）

### 6.2 配置说明

在 config.py 中可配置以下参数：

```python
TARGET_USER_ID = 'kaifulee'      # 目标用户ID
MAX_ANSWER_COUNT = 0              # 爬取数量限制，0表示无限制
LOGIN_WAIT_TIME = 60             # 登录等待时间（秒）
RANDOM_SLEEP_MIN = 3              # 翻页最小等待时间（秒）
RANDOM_SLEEP_MAX = 6              # 翻页最大等待时间（秒）
MAX_RETRY = 3                     # 最大重试次数
RETRY_INTERVAL = 5                # 重试间隔（秒）
```

---

## 7. 使用流程

### 7.1 首次运行

1. 安装依赖: `pip install -r requirements.txt`
2. 修改 config.py 中的 TARGET_USER_ID
3. 运行: `python spider.py`
4. 手动扫码登录
5. 等待采集完成

### 7.2 断点续传

如果程序中断（网络异常、手动停止等），再次运行时会：
- 自动加载 progress.json
- 跳过已爬取的回答
- 从上次中断的页码继续

### 7.3 数量限制

如果只需要爬取部分数据：
1. 在 config.py 中设置 MAX_ANSWER_COUNT = 5
2. 运行程序，爬取5条后自动停止

---

## 8. 伪代码流程

```
Init_Browser()

If not Logged_in:
    Wait_for_Manual_QR_Scan()

Load_Progress()  # 加载断点续传记录

Start_Network_Listener(target="api/v4/members/*/answers")

Go_to_URL(User_Answer_Page)

Loop:
    Scroll_Down()
    Response = Get_Listened_Data()
    
    If Response is Valid:
        Parse_JSON_to_List()
        
        For each answer:
            If answer_id not in progress:
                Append_to_CSV()
                Update_Progress()
                
            If count >= MAX_ANSWER_COUNT:
                Save_Progress()
                Exit()
    
    Button = Find_Element("Next Page")
    If Button exists:
        Random_Sleep(3, 6)
        Click(Button)
    Else:
        Break Loop

Save_Final_File()
Save_Progress()
Close_Browser()
```

---

## 9. 更新日志

### v1.1 (2026-01-26)

- 新增断点续传功能，支持异常后从上次位置继续
- 新增数量限制功能，可配置爬取指定数量的回答
- 优化代码结构，添加中文注释
- 改进异常处理，确保数据不丢失

### v1.0 (2026-01-26)

- 初始版本
- 实现基础爬虫功能
- 支持半自动登录
- 支持网络数据监听
- 支持反爬虫对抗
