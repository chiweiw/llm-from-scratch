# 知乎个人主页回答采集器

基于 DrissionPage 的知乎用户回答采集工具，支持半自动登录、网络数据监听、反爬虫对抗等功能。

## 功能特性

- 半自动登录（扫码登录，自动保存 Cookie）
- 网络数据监听（监听 API 响应，而非 DOM 解析）
- 模拟真人行为（随机延迟、滚动页面）
- 反爬虫对抗（403 错误处理、验证码手动处理）
- 断点续传（自动去重，异常后从上次位置继续）
- 数量限制（可配置爬取指定数量的回答）
- 数据实时保存（每页数据即时写入 CSV）

## 环境要求

- Python 3.8+
- Windows/macOS

## 安装步骤

1. 克隆或下载项目到本地

2. 安装依赖包：
```bash
pip install -r requirements.txt
```

## 使用方法

### 1. 配置目标用户

编辑 `config.py` 文件，修改 `TARGET_USER_ID` 为你要采集的用户 ID：

```python
TARGET_USER_ID = 'kaifulee'  # 修改为目标用户的 url_token
```

用户 ID 可以从知乎用户主页 URL 中获取，例如：
- `https://www.zhihu.com/people/kaifulee` → 用户 ID 为 `kaifulee`

### 2. 运行程序

```bash
python spider.py
```

### 3. 登录流程

首次运行时，程序会自动打开浏览器并跳转到知乎首页：

1. 等待浏览器窗口弹出
2. 手动点击"登录"按钮
3. 使用知乎 App 扫描二维码登录
4. 登录成功后，程序会自动保存 Cookie，下次运行无需重新登录

### 4. 采集过程

程序会自动：
- 访问目标用户的回答页面
- 监听网络 API 响应
- 模拟点击"下一页"按钮
- 每页数据实时保存到 CSV 文件

### 5. 输出文件

采集的数据会保存在 `output/` 目录下，文件名格式：
```
{user_id}_answers_{timestamp}.csv
```

例如：`kaifulee_answers_20260126_143025.csv`

## 数据字段

| 字段名 | 描述 |
|--------|------|
| Question Title | 问题标题 |
| Answer ID | 回答唯一 ID |
| Excerpt | 回答摘要/预览 |
| Content | 完整 HTML 内容 |
| Vote Count | 赞同数 |
| Comment Count | 评论数 |
| Create Time | 发布时间 |
| URL | 回答链接 |

## 反爬虫处理

### 403 错误

如果遇到 403 错误，程序会暂停并提示：
```
请切换IP（开关飞行模式）后按回车继续...
```

请按照提示操作：
1. 开启飞行模式
2. 等待 5-10 秒
3. 关闭飞行模式
4. 按回车键继续

### 验证码

如果遇到验证码（滑块或点选），程序会暂停并提示：
```
检测到可能的验证码，请手动处理...
处理完成后，程序将自动继续
```

请手动在浏览器上完成验证，然后按回车键继续。

## 配置参数

可以在 `config.py` 中调整以下参数：

```python
TARGET_USER_ID = 'kaifulee'      # 目标用户ID
MAX_ANSWER_COUNT = 0              # 爬取数量限制，0表示无限制
LOGIN_WAIT_TIME = 60             # 登录等待时间（秒）
RANDOM_SLEEP_MIN = 3              # 翻页最小等待时间（秒）
RANDOM_SLEEP_MAX = 6              # 翻页最大等待时间（秒）
MAX_RETRY = 3                     # 最大重试次数
RETRY_INTERVAL = 5                # 重试间隔（秒）
```

## 断点续传

程序支持断点续传功能，异常中断后可从上次位置继续：

- **进度文件**: `progress.json`（自动生成）
- **记录内容**: 已爬取的回答ID、当前页码、最后更新时间
- **使用方式**: 程序中断后再次运行，自动加载进度文件并继续采集
- **适用场景**: 网络异常、手动停止、程序崩溃等情况

## 数量限制

如果只需要爬取部分数据，可以设置数量限制：

1. 在 `config.py` 中设置 `MAX_ANSWER_COUNT = 5`
2. 运行程序，爬取5条后自动停止
3. 设置为 `0` 表示无限制，爬取所有回答

## 注意事项

1. **IP 切换**：建议使用手机热点，通过开关飞行模式切换 IP
2. **采集速度**：程序设置了随机延迟（3-6 秒），不要修改为过短时间
3. **数据备份**：每页数据会实时保存，即使程序中断也不会丢失已采集的数据
4. **登录失效**：如果提示 Cookie 失效，删除 `browser_data/` 目录后重新运行

## 项目结构

```
linkzhihu/
├── spider.py              # 主程序
├── config.py              # 配置文件
├── requirements.txt      # 依赖包
├── README.md             # 使用说明
├── browser_data/         # 浏览器数据（自动生成）
├── output/               # 输出目录（自动生成）
└── spider.log            # 运行日志（自动生成）
```

## 常见问题

### Q: 程序无法启动？

A: 检查是否正确安装了依赖包：`pip install -r requirements.txt`

### Q: 登录超时？

A: 增加 `config.py` 中的 `LOGIN_WAIT_TIME` 参数

### Q: 采集速度太慢？

A: 这是正常现象，程序设置了随机延迟以避免被检测，不建议修改

### Q: 如何采集其他用户？

A: 修改 `config.py` 中的 `TARGET_USER_ID` 参数

### Q: 如何查看运行日志？

A: 查看 `spider.log` 文件

## 技术栈

- **DrissionPage**: 浏览器自动化框架
- **Pandas**: 数据处理
- **Python 3.8+**: 编程语言

## 许可证

MIT License

## 免责声明

本工具仅供学习和研究使用，请勿用于商业用途或违反知乎服务条款的行为。使用本工具所产生的一切后果由使用者自行承担。
