âš™ï¸ ç¬¬å…­é˜¶æ®µï¼šå·¥ç¨‹ä¸æ•ˆç‡ï¼ˆç¨‹åºå‘˜ä¼˜åŠ¿ï¼‰
1ï¸âƒ£2ï¸âƒ£ LoRAï¼ˆ2021ï¼‰âœ…

Figure 1

Method éƒ¨åˆ†

ğŸ¯ ç›®æ ‡

ä¸ºä»€ä¹ˆä½ç§©é€‚åˆå¾®è°ƒ

1ï¸âƒ£3ï¸âƒ£ FlashAttentionï¼ˆ2022ï¼‰âš ï¸

çœ‹åŠ¨æœºå’Œ IO ä¼˜åŒ–æ€æƒ³


---

---
## ğŸ“š è®ºæ–‡é“¾æ¥ (Paper Links)

| Paper | Abstract | PDF |
|---|---|---|
| LoRA: Low-Rank Adaptation of Large Language Models | [Abstract](https://arxiv.org/abs/2106.09685) | [PDF](https://arxiv.org/pdf/2106.09685.pdf) |
| FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness | [Abstract](https://arxiv.org/abs/2205.14135) | [PDF](https://arxiv.org/pdf/2205.14135.pdf) |