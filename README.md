# LM From Scratch: 程序员的大模型原理学习笔记

欢迎！这是一个专为 **Python 开发者** 设计的大模型原理学习仓库。这里的目标不是成为算法专家，而是**通透理解底层逻辑**，建立起对 LLM 的直觉。

## 📂 目录导航

| 阶段 | 重点内容 | 核心实验/笔记 |
| :--- | :--- | :--- |
| **[00_基础知识](./00_基础知识)** | Tokenizer, Embedding, 基础名词 | `tokenizer_lab.py` |
| **[01_Transformer架构](./01_Transformer架构)** | Attention 机制, 整体架构 | `attention_toy.py` |
| **[02_GPT演进历程](./02_GPT演进历程)** | 从 GPT-1 到 GPT-2, 零样本学习 | 演进逻辑笔记 |
| **[03_规模化与GPT3](./03_规模化与GPT3)** | 规模定律, Few-shot Prompting | 规模法则理解 |
| **[04_对齐与强化学习RLHF](./04_对齐与强化学习RLHF)** | 指令微调, 人类反馈强化学习 | RLHF 三阶段图解 |
| **[05_推理与思维链CoT](./05_推理与思维链CoT)** | 思维链 (CoT), 逻辑推理 | Prompt 技巧 |
| **[06_高效微调LoRA](./06_高效微调LoRA)** | 参数高效微调 (PEFT), LoRA | 显存节省原理 |
| **[07_开源模型LLaMA](./07_开源模型LLaMA)** | LLaMA 家族, 开源生态 | 开源模型选型 |

## 🚀 如何开始？
1. **查阅名词**：先扫一遍 `00_基础知识/glossary.md`，遇到不认识的词随时回来查。
2. **动手实验**：
   - 运行 `python 00_基础知识/tokenizer_lab.py`
   - 运行 `python 01_Transformer架构/attention_toy.py`
3. **精读论文**：按照 `README_Route.md` 中的指引，只读论文的核心部分。

## 🧠 学习原则
- **不纠结数学**：看到公式先跳过，先看图和代码。
- **重逻辑闭环**：搞清楚“为什么要这么设计”比“这个参数是多少”更重要。
- **代码驱动**：复杂的算法，写一遍几行的小脚本就懂了。
