# Stage 2: Transformer → 语言模型 (GPT 系列的起点)

### 🎯 核心理解目标
- [ ] **Decoder-only 架构**：理解为什么 GPT 只用了 Transformer 的一半。
- [ ] **自回归生成 (Autoregressive)**：明白模型是如何“一个词一个词”蹦出来的。
- [ ] **预训练 (Pre-train) vs 微调 (Fine-tune)**：理解为什么在大规模数据上练过的模型，只需要一点点数据就能学会新任务。

### 📄 必读建议
- **GPT-1**: 重点看 Figure 1，理解从无监督预训练到有监督微调的过程。
- **GPT-2**: 核心关键词是 **Zero-shot**。理解为什么它不需要微调也能做任务（提示：Prompt 的雏形）。

### 💡 程序员视角
想象预训练是在“读遍天下书”建立常识，而微调是在“通过例题学习特定考试”。GPT-2 告诉我们，书读得够多，不看例题也能考及格。
