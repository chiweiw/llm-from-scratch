ğŸš€ ç¬¬ä¸‰é˜¶æ®µï¼šè§„æ¨¡ = èƒ½åŠ›æ¥æº
5ï¸âƒ£ Scaling Laws for Neural Language Modelsï¼ˆ2020ï¼‰âœ…

é˜…è¯»é¡ºåº

Abstract

Figure 1ã€Figure 2 âœ…

Section 3ï¼ˆScaling Lawï¼‰âš ï¸ï¼ˆä¸æ¨å…¬å¼ï¼‰

åªçœ‹å›¾ï¼Œä¸æ¨å¯¼

Loss vs å‚æ•°

Loss vs æ•°æ®

Loss vs è®¡ç®—é‡

ğŸ¯ ç›®æ ‡

ç†è§£ã€Œä¸æ˜¯æ¨¡å‹èªæ˜ï¼Œæ˜¯è§„æ¨¡è¶³å¤Ÿå¤§ã€

6ï¸âƒ£ GPT-3: Language Models are Few-Shot Learnersï¼ˆ2020ï¼‰âœ…

é˜…è¯»é¡ºåº

Abstract

Figure 1ï¼ˆFew-shotï¼‰âœ…

Section 2.3ï¼ˆTrainingï¼‰âš ï¸

å„ç§ä»»åŠ¡ç»“æœ âš ï¸

ğŸ¯ ç›®æ ‡

ç†è§£ Few-shot æ˜¯ Prompt æŠ€å·§ï¼Œè€Œä¸æ˜¯ç»“æ„å˜åŒ–


---

---
## ğŸ“š è®ºæ–‡é“¾æ¥ (Paper Links)

| Paper | Abstract | PDF |
|---|---|---|
| Scaling Laws for Neural Language Models | [Abstract](https://arxiv.org/abs/2001.08361) | [PDF](https://arxiv.org/pdf/2001.08361.pdf) |
| GPT-3: Language Models are Few-Shot Learners | [Abstract](https://arxiv.org/abs/2005.14165) | [PDF](https://arxiv.org/pdf/2005.14165.pdf) |